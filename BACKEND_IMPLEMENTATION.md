# åç«¯æ ¸å¿ƒæ¨¡å—è¯¦ç»†å®ç°æ–¹æ¡ˆ

## ğŸ“Š å½“å‰è¿›åº¦åˆ†æ

### âœ… å·²å®Œæˆæ¨¡å—ï¼ˆ70%ï¼‰

#### 1. ä»£ç å®ä½“æå–å™¨ï¼ˆextractorsï¼‰

```
src-tauri/src/tool_execution/codebase/extractors/
â”œâ”€â”€ mod.rs              âœ… æ ¸å¿ƒæ•°æ®ç»“æ„å®šä¹‰
â”œâ”€â”€ typescript.rs       âœ… TypeScript/TSX æå–å™¨
â”œâ”€â”€ vue.rs              âœ… Vue æ–‡ä»¶æå–å™¨
â””â”€â”€ type_utils.rs       âœ… ç±»å‹åˆ¤æ–­å·¥å…·
```

**å·²æœ‰æ•°æ®ç»“æ„**ï¼š

```rust
pub struct CodeEntity {
    pub id: String,           // "Component:Button"
    pub entity_type: String,  // "component", "function", "class"
    pub file: String,         // ç›¸å¯¹è·¯å¾„
    pub loc: LocationInfo,    // è¡Œå·èŒƒå›´
    pub raw_name: String,     // å®ä½“åç§°
}
```

**å·²æœ‰èƒ½åŠ›**ï¼š

- âœ… è§£æ TypeScript/TSX æ–‡ä»¶ï¼ˆåŸºäº tree-sitter ASTï¼‰
- âœ… è§£æ Vue æ–‡ä»¶ï¼ˆåŸºäºæ­£åˆ™è¡¨è¾¾å¼ï¼‰
- âœ… è¯†åˆ«ç»„ä»¶ã€å‡½æ•°ã€ç±»ã€å˜é‡
- âœ… æ™ºèƒ½ç±»å‹åˆ¤æ–­ï¼ˆåŒºåˆ†ç»„ä»¶å’Œæ™®é€šå‡½æ•°ï¼‰

**ç¼ºå¤±çš„å…³é”®ä¿¡æ¯**ï¼š

- âŒ **å®é™…ä»£ç å†…å®¹**ï¼ˆåªæœ‰ä½ç½®ï¼Œæ²¡æœ‰ä»£ç æ–‡æœ¬ï¼‰
- âŒ ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¯¼å…¥ã€å¯¼å‡ºã€æ³¨é‡Šï¼‰
- âŒ ä¾èµ–å…³ç³»ï¼ˆå¼•ç”¨äº†å“ªäº›å…¶ä»–å®ä½“ï¼‰

---

### âŒ å¾…å®ç°æ¨¡å—ï¼ˆ30%ï¼‰

```
src-tauri/src/tool_execution/codebase/
â”œâ”€â”€ chunking.rs         âŒ ä»£ç åˆ†å—å¢å¼ºæ¨¡å—
â”œâ”€â”€ embeddings.rs       âŒ å‘é‡åŒ–æ¨¡å—
â””â”€â”€ vector_store.rs     âŒ å‘é‡å­˜å‚¨æ¨¡å—ï¼ˆå¯é€‰ï¼Œåç»­é˜¶æ®µï¼‰
```

---

## ğŸ¯ å®ç°ç­–ç•¥ï¼šå¢é‡æ‰©å±•

æˆ‘ä»¬é‡‡ç”¨**æœ€å°ä¾µå…¥å¼**çš„è®¾è®¡ï¼Œåœ¨ç°æœ‰ä»£ç åŸºç¡€ä¸Šæ‰©å±•ï¼Œè€Œä¸æ˜¯é‡å†™ã€‚

### æ ¸å¿ƒæ€è·¯

```
å·²æœ‰: CodeEntityï¼ˆä½ç½®ä¿¡æ¯ï¼‰
      â†“ å¢å¼º
æ–°å¢: CodeChunkï¼ˆä½ç½® + ä»£ç å†…å®¹ + ä¸Šä¸‹æ–‡ï¼‰
      â†“ å‘é‡åŒ–
æ–°å¢: EmbeddedChunkï¼ˆChunk + å‘é‡ï¼‰
```

---

## ğŸ“¦ æ¨¡å— 1ï¼šä»£ç åˆ†å—å¢å¼ºæ¨¡å—ï¼ˆchunking.rsï¼‰

### ç›®æ ‡

å°†è½»é‡çº§çš„ `CodeEntity` è½¬æ¢ä¸ºåŒ…å«å®Œæ•´ä¿¡æ¯çš„ `CodeChunk`ã€‚

### è¯¦ç»†è®¾è®¡

#### 1.1 æ•°æ®ç»“æ„å®šä¹‰

```rust
// src-tauri/src/tool_execution/codebase/chunking.rs

use super::extractors::{CodeEntity, LocationInfo};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;

/// å¢å¼ºçš„ä»£ç å—ï¼ˆåŒ…å«å®Œæ•´ä¸Šä¸‹æ–‡ï¼‰
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CodeChunk {
    // ========== åŸºç¡€ä¿¡æ¯ï¼ˆç»§æ‰¿è‡ª CodeEntityï¼‰ ==========
    pub id: String,
    pub entity_type: String,
    pub file: String,
    pub raw_name: String,
    pub loc: LocationInfo,

    // ========== æ–°å¢ï¼šä»£ç å†…å®¹ ==========
    /// å®é™…çš„ä»£ç æ–‡æœ¬
    pub code: String,

    /// ä»£ç é•¿åº¦ï¼ˆå­—ç¬¦æ•°ï¼‰
    pub code_length: usize,

    // ========== æ–°å¢ï¼šä¸Šä¸‹æ–‡ä¿¡æ¯ ==========
    /// å¯¼å…¥çš„æ¨¡å—/åº“
    /// ä¾‹å¦‚: ["react", "@/utils/helpers", "./Button.css"]
    pub imports: Vec<String>,

    /// å¯¼å‡ºçš„å†…å®¹
    /// ä¾‹å¦‚: ["Button", "ButtonProps"]
    pub exports: Vec<String>,

    /// æ³¨é‡Šå†…å®¹ï¼ˆJSDocã€å•è¡Œã€å¤šè¡Œï¼‰
    pub comments: Vec<String>,

    /// ä¾èµ–çš„å…¶ä»–å®ä½“ ID
    /// ä¾‹å¦‚: ["Component:Icon", "Function:validateProps"]
    pub dependencies: Vec<String>,

    // ========== æ–°å¢ï¼šå…ƒæ•°æ® ==========
    /// ä»£ç å¤æ‚åº¦ï¼ˆç®€å•ä¼°ç®—ï¼šè¡Œæ•° + ç¼©è¿›å±‚çº§ï¼‰
    pub complexity: u32,

    /// æ˜¯å¦ä¸ºæµ‹è¯•ä»£ç 
    pub is_test: bool,

    /// æ–‡ä»¶çš„ç›¸å¯¹è·¯å¾„ï¼ˆå»é™¤ workspace å‰ç¼€ï¼‰
    pub relative_file: String,

    // ========== æ–°å¢ï¼šç”¨äº Embedding çš„æ ¼å¼åŒ–æ–‡æœ¬ ==========
    /// ç»è¿‡ä¼˜åŒ–çš„æ–‡æœ¬ï¼Œç”¨äºç”Ÿæˆå‘é‡
    /// åŒ…å«ç»“æ„åŒ–çš„å…ƒæ•°æ® + ä»£ç 
    pub embedding_text: String,
}

/// åˆ†å—ç»Ÿè®¡ä¿¡æ¯
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ChunkStats {
    pub total_chunks: usize,
    pub total_code_size: usize,
    pub avg_chunk_size: usize,
    pub by_type: HashMap<String, usize>,  // å„ç±»å‹æ•°é‡ç»Ÿè®¡
}
```

#### 1.2 æ ¸å¿ƒå®ç°ï¼šChunkBuilder

```rust
use std::fs;
use std::path::Path;
use regex::Regex;
use lazy_static::lazy_static;

lazy_static! {
    // å¯¼å…¥è¯­å¥æ­£åˆ™
    static ref IMPORT_REGEX: Regex = Regex::new(
        r#"import\s+(?:.*?\s+from\s+)?['"]([^'"]+)['"]"#
    ).unwrap();

    // å¯¼å‡ºè¯­å¥æ­£åˆ™
    static ref EXPORT_REGEX: Regex = Regex::new(
        r"export\s+(?:default\s+)?(?:const|let|var|function|class|interface|type)\s+(\w+)"
    ).unwrap();

    // JSDoc æ³¨é‡Šæ­£åˆ™
    static ref JSDOC_REGEX: Regex = Regex::new(
        r"/\*\*([\s\S]*?)\*/"
    ).unwrap();

    // å•è¡Œæ³¨é‡Šæ­£åˆ™
    static ref SINGLE_COMMENT_REGEX: Regex = Regex::new(
        r"//\s*(.+?)$"
    ).unwrap();
}

pub struct ChunkBuilder {
    workspace_root: String,
}

impl ChunkBuilder {
    pub fn new(workspace_root: String) -> Self {
        Self { workspace_root }
    }

    /// ä» CodeEntity åˆ›å»º CodeChunk
    pub fn build_chunk(
        &self,
        entity: CodeEntity,
    ) -> Result<CodeChunk, Box<dyn std::error::Error>> {
        // 1. è¯»å–æ–‡ä»¶å†…å®¹
        let full_path = Path::new(&self.workspace_root).join(&entity.file);
        let file_content = fs::read_to_string(&full_path)?;

        // 2. æå–æŒ‡å®šè¡ŒèŒƒå›´çš„ä»£ç 
        let code = self.extract_code_from_lines(
            &file_content,
            entity.loc.start_line,
            entity.loc.end_line,
        )?;

        // 3. åˆ†æä¸Šä¸‹æ–‡
        let imports = self.extract_imports(&code);
        let exports = self.extract_exports(&code);
        let comments = self.extract_comments(&code);

        // 4. è®¡ç®—å¤æ‚åº¦
        let complexity = self.calculate_complexity(&code);

        // 5. åˆ¤æ–­æ˜¯å¦ä¸ºæµ‹è¯•ä»£ç 
        let is_test = self.is_test_file(&entity.file) || self.is_test_code(&code);

        // 6. è®¡ç®—ç›¸å¯¹è·¯å¾„
        let relative_file = entity.file
            .strip_prefix(&self.workspace_root)
            .unwrap_or(&entity.file)
            .to_string();

        // 7. ç”Ÿæˆ embedding æ–‡æœ¬
        let embedding_text = self.format_for_embedding(
            &entity,
            &code,
            &imports,
            &exports,
            &comments,
            &relative_file,
        );

        Ok(CodeChunk {
            id: entity.id,
            entity_type: entity.entity_type,
            file: entity.file,
            raw_name: entity.raw_name,
            loc: entity.loc,
            code: code.clone(),
            code_length: code.len(),
            imports,
            exports,
            comments,
            dependencies: Vec::new(), // ç¬¬ä¸€é˜¶æ®µå…ˆç•™ç©ºï¼Œåç»­å¯ä»¥åˆ†æ
            complexity,
            is_test,
            relative_file,
            embedding_text,
        })
    }

    /// ä»æ–‡ä»¶å†…å®¹æå–æŒ‡å®šè¡ŒèŒƒå›´çš„ä»£ç 
    fn extract_code_from_lines(
        &self,
        content: &str,
        start_line: usize,
        end_line: usize,
    ) -> Result<String, Box<dyn std::error::Error>> {
        let lines: Vec<&str> = content.lines().collect();

        // æ ¡éªŒè¡Œå·èŒƒå›´
        if start_line == 0 || start_line > lines.len() {
            return Err(format!("Invalid start_line: {}", start_line).into());
        }

        // è½¬æ¢ä¸º 0-based ç´¢å¼•
        let start_idx = start_line - 1;
        let end_idx = std::cmp::min(end_line, lines.len());

        // æå–ä»£ç 
        let code = lines[start_idx..end_idx].join("\n");

        Ok(code)
    }

    /// æå–å¯¼å…¥è¯­å¥
    fn extract_imports(&self, code: &str) -> Vec<String> {
        IMPORT_REGEX
            .captures_iter(code)
            .filter_map(|cap| cap.get(1).map(|m| m.as_str().to_string()))
            .collect()
    }

    /// æå–å¯¼å‡ºå†…å®¹
    fn extract_exports(&self, code: &str) -> Vec<String> {
        EXPORT_REGEX
            .captures_iter(code)
            .filter_map(|cap| cap.get(1).map(|m| m.as_str().to_string()))
            .collect()
    }

    /// æå–æ³¨é‡Š
    fn extract_comments(&self, code: &str) -> Vec<String> {
        let mut comments = Vec::new();

        // æå– JSDoc
        for cap in JSDOC_REGEX.captures_iter(code) {
            if let Some(comment) = cap.get(1) {
                let cleaned = comment.as_str()
                    .lines()
                    .map(|line| line.trim_start_matches('*').trim())
                    .filter(|line| !line.is_empty())
                    .collect::<Vec<_>>()
                    .join(" ");
                comments.push(cleaned);
            }
        }

        // æå–å•è¡Œæ³¨é‡Šï¼ˆåªä¿ç•™æœ‰æ„ä¹‰çš„æ³¨é‡Šï¼‰
        for cap in SINGLE_COMMENT_REGEX.captures_iter(code) {
            if let Some(comment) = cap.get(1) {
                let text = comment.as_str().trim();
                // è¿‡æ»¤æ‰å¤ªçŸ­æˆ–çº¯ç¬¦å·çš„æ³¨é‡Š
                if text.len() > 5 && text.chars().any(|c| c.is_alphabetic()) {
                    comments.push(text.to_string());
                }
            }
        }

        comments
    }

    /// è®¡ç®—ä»£ç å¤æ‚åº¦ï¼ˆç®€åŒ–ç‰ˆï¼‰
    fn calculate_complexity(&self, code: &str) -> u32 {
        let line_count = code.lines().count() as u32;

        // ç»Ÿè®¡æ§åˆ¶æµå…³é”®å­—
        let control_flow_keywords = [
            "if", "else", "for", "while", "switch", "case",
            "try", "catch", "async", "await"
        ];

        let mut control_flow_count = 0u32;
        for keyword in &control_flow_keywords {
            control_flow_count += code.matches(keyword).count() as u32;
        }

        // ç®€å•çš„å¤æ‚åº¦å…¬å¼
        line_count + control_flow_count * 2
    }

    /// åˆ¤æ–­æ˜¯å¦ä¸ºæµ‹è¯•æ–‡ä»¶
    fn is_test_file(&self, file_path: &str) -> bool {
        let path = file_path.to_lowercase();
        path.contains("test") || path.contains("spec") || path.contains("__tests__")
    }

    /// åˆ¤æ–­æ˜¯å¦ä¸ºæµ‹è¯•ä»£ç 
    fn is_test_code(&self, code: &str) -> bool {
        let test_keywords = ["describe", "it(", "test(", "expect("];
        test_keywords.iter().any(|keyword| code.contains(keyword))
    }

    /// æ ¼å¼åŒ–ä¸ºé€‚åˆ embedding çš„æ–‡æœ¬
    /// è¿™æ˜¯æœ€å…³é”®çš„å‡½æ•°ï¼Œå†³å®šäº† AI å¦‚ä½•ç†è§£ä»£ç 
    fn format_for_embedding(
        &self,
        entity: &CodeEntity,
        code: &str,
        imports: &[String],
        exports: &[String],
        comments: &[String],
        relative_file: &str,
    ) -> String {
        let mut parts = Vec::new();

        // 1. æ–‡ä»¶è·¯å¾„ï¼ˆå¸®åŠ© AI ç†è§£ä»£ç ä½ç½®ï¼‰
        parts.push(format!("File: {}", relative_file));

        // 2. å®ä½“ç±»å‹å’Œåç§°
        parts.push(format!("Type: {} | Name: {}", entity.entity_type, entity.raw_name));

        // 3. ä½ç½®ä¿¡æ¯
        parts.push(format!(
            "Location: Lines {}-{}",
            entity.loc.start_line,
            entity.loc.end_line
        ));

        // 4. å¯¼å…¥ä¿¡æ¯ï¼ˆæ˜¾ç¤ºä¾èµ–ï¼‰
        if !imports.is_empty() {
            parts.push(format!("Imports: {}", imports.join(", ")));
        }

        // 5. å¯¼å‡ºä¿¡æ¯
        if !exports.is_empty() {
            parts.push(format!("Exports: {}", exports.join(", ")));
        }

        // 6. æ³¨é‡Šï¼ˆé‡è¦çš„è¯­ä¹‰ä¿¡æ¯ï¼‰
        if !comments.is_empty() {
            parts.push(format!("Comments: {}", comments.join(" | ")));
        }

        // 7. åˆ†éš”ç¬¦
        parts.push("---".to_string());

        // 8. å®é™…ä»£ç 
        parts.push(code.to_string());

        parts.join("\n")
    }

    /// æ‰¹é‡æ„å»º chunks
    pub fn build_chunks(
        &self,
        entities: Vec<CodeEntity>,
    ) -> Result<(Vec<CodeChunk>, ChunkStats), Box<dyn std::error::Error>> {
        let mut chunks = Vec::new();
        let mut stats = ChunkStats {
            total_chunks: 0,
            total_code_size: 0,
            avg_chunk_size: 0,
            by_type: HashMap::new(),
        };

        for entity in entities {
            match self.build_chunk(entity) {
                Ok(chunk) => {
                    // æ›´æ–°ç»Ÿè®¡
                    stats.total_code_size += chunk.code_length;
                    *stats.by_type.entry(chunk.entity_type.clone()).or_insert(0) += 1;

                    chunks.push(chunk);
                }
                Err(e) => {
                    eprintln!("âš ï¸ Failed to build chunk: {}", e);
                    // ç»§ç»­å¤„ç†å…¶ä»– entity
                }
            }
        }

        stats.total_chunks = chunks.len();
        stats.avg_chunk_size = if stats.total_chunks > 0 {
            stats.total_code_size / stats.total_chunks
        } else {
            0
        };

        Ok((chunks, stats))
    }
}
```

#### 1.3 ä½¿ç”¨ç¤ºä¾‹

```rust
use crate::tool_execution::codebase::extractors::TypeScriptExtractor;
use crate::tool_execution::codebase::chunking::ChunkBuilder;

#[tokio::test]
async fn test_chunking() {
    // 1. æå–å®ä½“ï¼ˆå·²æœ‰åŠŸèƒ½ï¼‰
    let mut extractor = TypeScriptExtractor::new(false).unwrap();
    let entities = extractor.extract(
        "/path/to/file.ts",
        "/path/to/workspace"
    ).unwrap();

    // 2. æ„å»º chunksï¼ˆæ–°åŠŸèƒ½ï¼‰
    let builder = ChunkBuilder::new("/path/to/workspace".to_string());
    let (chunks, stats) = builder.build_chunks(entities).unwrap();

    println!("âœ… ç”Ÿæˆäº† {} ä¸ªä»£ç å—", chunks.len());
    println!("ğŸ“Š å¹³å‡å¤§å°: {} å­—ç¬¦", stats.avg_chunk_size);
    println!("ğŸ“‹ ç±»å‹åˆ†å¸ƒ: {:?}", stats.by_type);

    // 3. æŸ¥çœ‹ç¬¬ä¸€ä¸ª chunk çš„ embedding æ–‡æœ¬
    if let Some(chunk) = chunks.first() {
        println!("\nğŸ“„ Embedding æ–‡æœ¬ç¤ºä¾‹:\n{}", chunk.embedding_text);
    }
}
```

---

## ğŸ§  æ¨¡å— 2ï¼šå‘é‡åŒ–æ¨¡å—ï¼ˆembeddings.rsï¼‰

### ç›®æ ‡

è°ƒç”¨ OpenAI API å°† `CodeChunk` è½¬æ¢ä¸ºå‘é‡ã€‚

### è¯¦ç»†è®¾è®¡

#### 2.1 æ•°æ®ç»“æ„å®šä¹‰

```rust
// src-tauri/src/tool_execution/codebase/embeddings.rs

use super::chunking::CodeChunk;
use serde::{Deserialize, Serialize};
use reqwest::Client;
use std::collections::HashMap;
use std::time::Duration;

/// å¸¦å‘é‡çš„ä»£ç å—
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EmbeddedChunk {
    /// åŸå§‹ chunk
    pub chunk: CodeChunk,

    /// å‘é‡ï¼ˆ1536 ç»´ï¼‰
    pub embedding: Vec<f32>,
}

/// Embedding ç»Ÿè®¡ä¿¡æ¯
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct EmbeddingStats {
    /// æ€»å…±å¤„ç†çš„ chunk æ•°é‡
    pub total_chunks: usize,

    /// æ€»å…±æ¶ˆè€—çš„ tokens
    pub total_tokens: usize,

    /// é¢„ä¼°æˆæœ¬ï¼ˆç¾å…ƒï¼‰
    pub estimated_cost: f64,

    /// ç¼“å­˜å‘½ä¸­æ¬¡æ•°
    pub cache_hits: usize,

    /// API è°ƒç”¨æ¬¡æ•°
    pub api_calls: usize,

    /// è€—æ—¶ï¼ˆç§’ï¼‰
    pub duration_secs: f64,
}

/// OpenAI API è¯·æ±‚
#[derive(Serialize)]
struct EmbeddingRequest {
    model: String,
    input: Vec<String>,
    encoding_format: String,
}

/// OpenAI API å“åº”
#[derive(Deserialize)]
struct EmbeddingResponse {
    data: Vec<EmbeddingData>,
    usage: Usage,
}

#[derive(Deserialize)]
struct EmbeddingData {
    embedding: Vec<f32>,
    index: usize,
}

#[derive(Deserialize)]
struct Usage {
    total_tokens: usize,
}
```

#### 2.2 æ ¸å¿ƒå®ç°ï¼šEmbeddingsClient

```rust
use std::time::Instant;
use tokio::time::sleep;

pub struct EmbeddingsClient {
    api_key: String,
    client: Client,
    model: String,
    batch_size: usize,
    cache: HashMap<String, Vec<f32>>,  // ç®€å•çš„å†…å­˜ç¼“å­˜
}

impl EmbeddingsClient {
    /// åˆ›å»ºå®¢æˆ·ç«¯
    pub fn new(api_key: String) -> Self {
        Self {
            api_key,
            client: Client::builder()
                .timeout(Duration::from_secs(60))
                .build()
                .unwrap(),
            model: "text-embedding-3-small".to_string(),
            batch_size: 100,  // OpenAI æœ€å¤šæ”¯æŒ 2048ï¼Œä½†æˆ‘ä»¬ç”¨ 100 æ›´ç¨³å®š
            cache: HashMap::new(),
        }
    }

    /// ä¸ºå•ä¸ª chunk ç”Ÿæˆå‘é‡
    pub async fn embed_chunk(
        &mut self,
        chunk: &CodeChunk,
    ) -> Result<EmbeddedChunk, Box<dyn std::error::Error>> {
        // æ£€æŸ¥ç¼“å­˜
        let cache_key = self.compute_cache_key(&chunk.embedding_text);
        if let Some(embedding) = self.cache.get(&cache_key) {
            return Ok(EmbeddedChunk {
                chunk: chunk.clone(),
                embedding: embedding.clone(),
            });
        }

        // è°ƒç”¨ API
        let embeddings = self.call_api(vec![chunk.embedding_text.clone()]).await?;

        // æ›´æ–°ç¼“å­˜
        if let Some(embedding) = embeddings.first() {
            self.cache.insert(cache_key, embedding.clone());

            Ok(EmbeddedChunk {
                chunk: chunk.clone(),
                embedding: embedding.clone(),
            })
        } else {
            Err("No embedding returned".into())
        }
    }

    /// æ‰¹é‡ç”Ÿæˆå‘é‡ï¼ˆæ¨èä½¿ç”¨ï¼‰
    pub async fn embed_chunks(
        &mut self,
        chunks: Vec<CodeChunk>,
    ) -> Result<(Vec<EmbeddedChunk>, EmbeddingStats), Box<dyn std::error::Error>> {
        let start_time = Instant::now();
        let mut embedded_chunks = Vec::new();
        let mut stats = EmbeddingStats {
            total_chunks: chunks.len(),
            total_tokens: 0,
            estimated_cost: 0.0,
            cache_hits: 0,
            api_calls: 0,
            duration_secs: 0.0,
        };

        // åˆ†æ‰¹å¤„ç†
        for batch in chunks.chunks(self.batch_size) {
            println!("ğŸ§  å¤„ç†æ‰¹æ¬¡ ({} chunks)...", batch.len());

            // åˆ†ç¦»ç¼“å­˜å‘½ä¸­å’Œéœ€è¦è°ƒç”¨ API çš„
            let mut texts_to_embed = Vec::new();
            let mut indices_to_embed = Vec::new();

            for (idx, chunk) in batch.iter().enumerate() {
                let cache_key = self.compute_cache_key(&chunk.embedding_text);

                if let Some(embedding) = self.cache.get(&cache_key) {
                    // ç¼“å­˜å‘½ä¸­
                    embedded_chunks.push(EmbeddedChunk {
                        chunk: chunk.clone(),
                        embedding: embedding.clone(),
                    });
                    stats.cache_hits += 1;
                } else {
                    // éœ€è¦è°ƒç”¨ API
                    texts_to_embed.push(chunk.embedding_text.clone());
                    indices_to_embed.push(idx);
                }
            }

            // è°ƒç”¨ API
            if !texts_to_embed.is_empty() {
                match self.call_api_with_retry(texts_to_embed.clone()).await {
                    Ok((embeddings, tokens)) => {
                        // æ›´æ–°ç»Ÿè®¡
                        stats.api_calls += 1;
                        stats.total_tokens += tokens;
                        stats.estimated_cost += (tokens as f64 / 1000.0) * 0.00002;

                        // ä¿å­˜ç»“æœ
                        for (i, embedding) in embeddings.iter().enumerate() {
                            let chunk_idx = indices_to_embed[i];
                            let chunk = &batch[chunk_idx];

                            // æ›´æ–°ç¼“å­˜
                            let cache_key = self.compute_cache_key(&chunk.embedding_text);
                            self.cache.insert(cache_key, embedding.clone());

                            // æ·»åŠ åˆ°ç»“æœ
                            embedded_chunks.push(EmbeddedChunk {
                                chunk: chunk.clone(),
                                embedding: embedding.clone(),
                            });
                        }
                    }
                    Err(e) => {
                        eprintln!("âŒ API è°ƒç”¨å¤±è´¥: {}", e);
                        // ç»§ç»­å¤„ç†ä¸‹ä¸€æ‰¹
                    }
                }

                // é¿å…è§¦å‘é€Ÿç‡é™åˆ¶
                sleep(Duration::from_millis(200)).await;
            }

            // æ‰“å°è¿›åº¦
            let progress = (embedded_chunks.len() as f64 / chunks.len() as f64) * 100.0;
            println!("ğŸ“Š è¿›åº¦: {:.1}% ({}/{})", progress, embedded_chunks.len(), chunks.len());
        }

        stats.duration_secs = start_time.elapsed().as_secs_f64();

        println!("\nâœ… å‘é‡åŒ–å®Œæˆ!");
        println!("  æ€»æ•°: {} chunks", stats.total_chunks);
        println!("  ç¼“å­˜å‘½ä¸­: {}", stats.cache_hits);
        println!("  API è°ƒç”¨: {}", stats.api_calls);
        println!("  æ€» tokens: {}", stats.total_tokens);
        println!("  é¢„ä¼°æˆæœ¬: ${:.4}", stats.estimated_cost);
        println!("  è€—æ—¶: {:.2}s", stats.duration_secs);

        Ok((embedded_chunks, stats))
    }

    /// è°ƒç”¨ OpenAI APIï¼ˆå¸¦é‡è¯•ï¼‰
    async fn call_api_with_retry(
        &self,
        texts: Vec<String>,
    ) -> Result<(Vec<Vec<f32>>, usize), Box<dyn std::error::Error>> {
        let max_retries = 3;
        let mut last_error = None;

        for attempt in 1..=max_retries {
            match self.call_api(texts.clone()).await {
                Ok(result) => return Ok(result),
                Err(e) => {
                    eprintln!("âš ï¸ API è°ƒç”¨å¤±è´¥ (å°è¯• {}/{}): {}", attempt, max_retries, e);
                    last_error = Some(e);

                    // æŒ‡æ•°é€€é¿
                    let delay = Duration::from_secs(2u64.pow(attempt as u32));
                    sleep(delay).await;
                }
            }
        }

        Err(last_error.unwrap_or_else(|| "Unknown error".into()))
    }

    /// è°ƒç”¨ OpenAI APIï¼ˆå•æ¬¡ï¼‰
    async fn call_api(
        &self,
        texts: Vec<String>,
    ) -> Result<(Vec<Vec<f32>>, usize), Box<dyn std::error::Error>> {
        let request = EmbeddingRequest {
            model: self.model.clone(),
            input: texts,
            encoding_format: "float".to_string(),
        };

        let response = self.client
            .post("https://api.openai.com/v1/embeddings")
            .header("Authorization", format!("Bearer {}", self.api_key))
            .header("Content-Type", "application/json")
            .json(&request)
            .send()
            .await?;

        // æ£€æŸ¥çŠ¶æ€ç 
        if !response.status().is_success() {
            let status = response.status();
            let error_text = response.text().await?;
            return Err(format!("API error ({}): {}", status, error_text).into());
        }

        // è§£æå“åº”
        let embedding_response: EmbeddingResponse = response.json().await?;

        // æå–å‘é‡ï¼ˆæŒ‰ index æ’åºï¼‰
        let mut embeddings_with_index: Vec<_> = embedding_response.data
            .into_iter()
            .map(|d| (d.index, d.embedding))
            .collect();
        embeddings_with_index.sort_by_key(|(idx, _)| *idx);

        let embeddings: Vec<Vec<f32>> = embeddings_with_index
            .into_iter()
            .map(|(_, emb)| emb)
            .collect();

        Ok((embeddings, embedding_response.usage.total_tokens))
    }

    /// è®¡ç®—ç¼“å­˜é”®ï¼ˆä½¿ç”¨ä»£ç çš„ hashï¼‰
    fn compute_cache_key(&self, text: &str) -> String {
        use std::collections::hash_map::DefaultHasher;
        use std::hash::{Hash, Hasher};

        let mut hasher = DefaultHasher::new();
        text.hash(&mut hasher);
        format!("{:x}", hasher.finish())
    }
}
```

#### 2.3 ä½¿ç”¨ç¤ºä¾‹

```rust
#[tokio::test]
async fn test_embeddings() {
    // å‡è®¾å·²ç»æœ‰äº† chunks
    let chunks = vec![/* ... */];

    // åˆ›å»ºå®¢æˆ·ç«¯
    let api_key = std::env::var("OPENAI_API_KEY").unwrap();
    let mut client = EmbeddingsClient::new(api_key);

    // æ‰¹é‡ç”Ÿæˆå‘é‡
    let (embedded_chunks, stats) = client.embed_chunks(chunks).await.unwrap();

    // æŸ¥çœ‹ç»“æœ
    println!("å‘é‡ç»´åº¦: {}", embedded_chunks[0].embedding.len()); // 1536
    println!("æ€»æˆæœ¬: ${:.4}", stats.estimated_cost);
}
```

---

## ğŸ”— é›†æˆï¼šå®Œæ•´å·¥ä½œæµ

### 3.1 åˆ›å»ºç»Ÿä¸€çš„ Indexer

```rust
// src-tauri/src/tool_execution/codebase/mod.rs

pub mod extractors;
pub mod chunking;
pub mod embeddings;

pub use extractors::{CodeEntity, TypeScriptExtractor, VueExtractor};
pub use chunking::{CodeChunk, ChunkBuilder};
pub use embeddings::{EmbeddedChunk, EmbeddingsClient};

use std::path::Path;
use walkdir::WalkDir;

/// ä»£ç åº“ç´¢å¼•å™¨ï¼ˆåè°ƒæ‰€æœ‰æ¨¡å—ï¼‰
pub struct CodebaseIndexer {
    workspace_root: String,
    ts_extractor: TypeScriptExtractor,
    vue_extractor: VueExtractor,
    chunk_builder: ChunkBuilder,
    embeddings_client: EmbeddingsClient,
}

impl CodebaseIndexer {
    pub fn new(workspace_root: String, openai_api_key: String) -> Result<Self, Box<dyn std::error::Error>> {
        Ok(Self {
            workspace_root: workspace_root.clone(),
            ts_extractor: TypeScriptExtractor::new(false)?,
            vue_extractor: VueExtractor::new(),
            chunk_builder: ChunkBuilder::new(workspace_root),
            embeddings_client: EmbeddingsClient::new(openai_api_key),
        })
    }

    /// å®Œæ•´çš„ç´¢å¼•æµç¨‹
    pub async fn index_workspace(&mut self) -> Result<Vec<EmbeddedChunk>, Box<dyn std::error::Error>> {
        println!("ğŸš€ å¼€å§‹ç´¢å¼•å·¥ä½œåŒº: {}", self.workspace_root);

        // æ­¥éª¤ 1: æ‰«ææ–‡ä»¶
        println!("\nğŸ“ æ­¥éª¤ 1/4: æ‰«ææ–‡ä»¶...");
        let files = self.scan_files()?;
        println!("  å‘ç° {} ä¸ªæ–‡ä»¶", files.len());

        // æ­¥éª¤ 2: æå–å®ä½“
        println!("\nğŸ” æ­¥éª¤ 2/4: æå–ä»£ç å®ä½“...");
        let entities = self.extract_entities(&files)?;
        println!("  æå– {} ä¸ªå®ä½“", entities.len());

        // æ­¥éª¤ 3: æ„å»º chunks
        println!("\nğŸ“¦ æ­¥éª¤ 3/4: æ„å»ºä»£ç å—...");
        let (chunks, chunk_stats) = self.chunk_builder.build_chunks(entities)?;
        println!("  ç”Ÿæˆ {} ä¸ªä»£ç å—", chunks.len());
        println!("  å¹³å‡å¤§å°: {} å­—ç¬¦", chunk_stats.avg_chunk_size);

        // æ­¥éª¤ 4: å‘é‡åŒ–
        println!("\nğŸ§  æ­¥éª¤ 4/4: ç”Ÿæˆå‘é‡...");
        let (embedded_chunks, embed_stats) = self.embeddings_client.embed_chunks(chunks).await?;

        println!("\nâœ… ç´¢å¼•å®Œæˆ!");
        println!("  æ€»å…±: {} ä¸ªå‘é‡åŒ–ä»£ç å—", embedded_chunks.len());
        println!("  æˆæœ¬: ${:.4}", embed_stats.estimated_cost);

        Ok(embedded_chunks)
    }

    /// æ‰«ææ–‡ä»¶
    fn scan_files(&self) -> Result<Vec<String>, Box<dyn std::error::Error>> {
        let mut files = Vec::new();

        for entry in WalkDir::new(&self.workspace_root)
            .follow_links(false)
            .into_iter()
            .filter_map(|e| e.ok())
        {
            let path = entry.path();

            if path.is_file() {
                if let Some(ext) = path.extension() {
                    if matches!(ext.to_str(), Some("ts") | Some("tsx") | Some("js") | Some("jsx") | Some("vue")) {
                        // æ’é™¤ node_modules ç­‰
                        let path_str = path.to_string_lossy();
                        if !path_str.contains("node_modules") && !path_str.contains("dist") {
                            files.push(path.to_string_lossy().to_string());
                        }
                    }
                }
            }
        }

        Ok(files)
    }

    /// æå–å®ä½“
    fn extract_entities(&mut self, files: &[String]) -> Result<Vec<CodeEntity>, Box<dyn std::error::Error>> {
        let mut all_entities = Vec::new();

        for file in files {
            let entities = if file.ends_with(".vue") {
                self.vue_extractor.extract(file, &self.workspace_root)?
            } else if file.ends_with(".tsx") || file.ends_with(".jsx") {
                self.ts_extractor = TypeScriptExtractor::new(true)?;
                self.ts_extractor.extract(file, &self.workspace_root)?
            } else {
                self.ts_extractor = TypeScriptExtractor::new(false)?;
                self.ts_extractor.extract(file, &self.workspace_root)?
            };

            all_entities.extend(entities);
        }

        Ok(all_entities)
    }
}
```

### 3.2 å®Œæ•´ä½¿ç”¨ç¤ºä¾‹

```rust
#[tokio::test]
async fn test_full_workflow() {
    // åˆ›å»ºç´¢å¼•å™¨
    let workspace = "/path/to/your/project".to_string();
    let api_key = std::env::var("OPENAI_API_KEY").unwrap();

    let mut indexer = CodebaseIndexer::new(workspace, api_key).unwrap();

    // æ‰§è¡Œå®Œæ•´ç´¢å¼•
    let embedded_chunks = indexer.index_workspace().await.unwrap();

    // æ­¤æ—¶ä½ æœ‰äº†æ‰€æœ‰ä»£ç å—çš„å‘é‡
    // å¯ä»¥ä¿å­˜åˆ°æ–‡ä»¶ï¼Œæˆ–è€…åç»­å­˜å…¥ Qdrant
    println!("âœ… ç”Ÿæˆäº† {} ä¸ªå‘é‡åŒ–ä»£ç å—", embedded_chunks.len());

    // ç¤ºä¾‹ï¼šä¿å­˜åˆ° JSON
    let json = serde_json::to_string_pretty(&embedded_chunks).unwrap();
    std::fs::write("embedded_chunks.json", json).unwrap();
}
```

---

## ğŸ“‹ å®æ–½æ¸…å•

### Week 1: ä»£ç åˆ†å—æ¨¡å—

- [ ] **Day 1-2**: å®ç° CodeChunk æ•°æ®ç»“æ„å’Œ ChunkBuilder
  - [ ] åˆ›å»º `chunking.rs` æ–‡ä»¶
  - [ ] å®ç° `extract_code_from_lines`
  - [ ] å®ç° `extract_imports` å’Œ `extract_exports`
  - [ ] å®ç° `extract_comments`
- [ ] **Day 3**: å®ç° embedding æ–‡æœ¬æ ¼å¼åŒ–

  - [ ] è®¾è®¡æœ€ä¼˜çš„æ ¼å¼åŒ–æ¨¡æ¿
  - [ ] å®ç° `format_for_embedding`
  - [ ] ç¼–å†™å•å…ƒæµ‹è¯•

- [ ] **Day 4**: é›†æˆæµ‹è¯•
  - [ ] æµ‹è¯•ä¸ç°æœ‰ TypeScriptExtractor çš„é›†æˆ
  - [ ] æµ‹è¯•ä¸ VueExtractor çš„é›†æˆ
  - [ ] ä¼˜åŒ–æ€§èƒ½

### Week 2: å‘é‡åŒ–æ¨¡å—

- [ ] **Day 1-2**: å®ç° EmbeddingsClient

  - [ ] åˆ›å»º `embeddings.rs` æ–‡ä»¶
  - [ ] å®ç° API è°ƒç”¨é€»è¾‘
  - [ ] å®ç°é”™è¯¯å¤„ç†å’Œé‡è¯•

- [ ] **Day 3**: å®ç°æ‰¹é‡å¤„ç†å’Œç¼“å­˜

  - [ ] æ‰¹é‡ API è°ƒç”¨
  - [ ] å†…å­˜ç¼“å­˜
  - [ ] è¿›åº¦æ˜¾ç¤º

- [ ] **Day 4**: é›†æˆæµ‹è¯•
  - [ ] ç«¯åˆ°ç«¯æµ‹è¯•
  - [ ] æ€§èƒ½æµ‹è¯•
  - [ ] æˆæœ¬æµ‹è¯•

### Week 3: é›†æˆå’Œä¼˜åŒ–

- [ ] **Day 1-2**: åˆ›å»º CodebaseIndexer
  - [ ] å®ç°å®Œæ•´å·¥ä½œæµ
  - [ ] æ·»åŠ é…ç½®é€‰é¡¹
- [ ] **Day 3-4**: ä¼˜åŒ–å’Œæ–‡æ¡£
  - [ ] æ€§èƒ½ä¼˜åŒ–
  - [ ] ç¼–å†™ä½¿ç”¨æ–‡æ¡£
  - [ ] å‡†å¤‡ç¤ºä¾‹

---

## ğŸ”§ é…ç½®å»ºè®®

### Cargo.toml æ›´æ–°

```toml
[dependencies]
# ç°æœ‰ä¾èµ–ä¿æŒä¸å˜...

# æ–°å¢ä¾èµ–ï¼ˆå¦‚æœè¿˜æ²¡æœ‰çš„è¯ï¼‰
tokio = { version = "1.0", features = ["full"] }
reqwest = { version = "0.11", features = ["json"] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
regex = "1.10"
lazy_static = "1.4"
walkdir = "2.3"
anyhow = "1.0"
```

### ç¯å¢ƒå˜é‡é…ç½®

```bash
# .env
OPENAI_API_KEY=sk-your-key-here
WORKSPACE_ROOT=/path/to/your/project
```

---

## ğŸ“Š é¢„æœŸè¾“å‡ºç¤ºä¾‹

### CodeChunk ç¤ºä¾‹

```json
{
  "id": "Component:Button",
  "entity_type": "component",
  "file": "src/components/Button.tsx",
  "raw_name": "Button",
  "loc": { "start_line": 10, "end_line": 25 },
  "code": "export function Button({ onClick, children }) {...}",
  "code_length": 156,
  "imports": ["react", "./Button.css"],
  "exports": ["Button", "ButtonProps"],
  "comments": ["Reusable button component with various styles"],
  "complexity": 8,
  "is_test": false,
  "relative_file": "src/components/Button.tsx",
  "embedding_text": "File: src/components/Button.tsx\nType: component | Name: Button\n..."
}
```

### EmbeddedChunk ç¤ºä¾‹

```json
{
  "chunk": { /* CodeChunk */ },
  "embedding": [0.023, -0.145, 0.567, ..., 0.012]  // 1536 ä¸ªæ•°å­—
}
```

---

## â±ï¸ æ—¶é—´ä¼°ç®—

| æ¨¡å—         | é¢„è®¡æ—¶é—´    | å…³é”®ä»»åŠ¡                     |
| ------------ | ----------- | ---------------------------- |
| ä»£ç åˆ†å—æ¨¡å— | 3-4 å¤©      | ChunkBuilder å®ç° + æµ‹è¯•     |
| å‘é‡åŒ–æ¨¡å—   | 3-4 å¤©      | EmbeddingsClient + æ‰¹é‡å¤„ç†  |
| é›†æˆå’Œæµ‹è¯•   | 2-3 å¤©      | CodebaseIndexer + ç«¯åˆ°ç«¯æµ‹è¯• |
| **æ€»è®¡**     | **8-11 å¤©** | çº¦ 2 å‘¨                      |

---

## ğŸ’° æˆæœ¬ä¼°ç®—

**ç¤ºä¾‹é¡¹ç›®**ï¼ˆä¸­ç­‰è§„æ¨¡ï¼‰ï¼š

- æ–‡ä»¶æ•°ï¼š500
- å®ä½“æ•°ï¼š2500
- å¹³å‡ embedding æ–‡æœ¬ï¼š200 tokens/å®ä½“
- æ€» tokensï¼š500,000

**æˆæœ¬**ï¼š

- æ¨¡å‹ï¼štext-embedding-3-small ($0.00002/1K tokens)
- æ€»æˆæœ¬ï¼š500,000 Ã· 1000 Ã— $0.00002 = **$0.01**ï¼ˆçº¦ 0.07 å…ƒï¼‰

---

## ğŸ¯ éªŒæ”¶æ ‡å‡†

### ä»£ç åˆ†å—æ¨¡å—

- âœ… èƒ½ä» CodeEntity ç”Ÿæˆ CodeChunk
- âœ… æ­£ç¡®æå–ä»£ç å†…å®¹ï¼ˆä¿æŒæ ¼å¼ï¼‰
- âœ… å‡†ç¡®æå– imports å’Œ exports
- âœ… embedding_text æ ¼å¼åˆç†
- âœ… å•å…ƒæµ‹è¯•è¦†ç›–ç‡ > 80%

### å‘é‡åŒ–æ¨¡å—

- âœ… èƒ½æˆåŠŸè°ƒç”¨ OpenAI API
- âœ… æ‰¹é‡å¤„ç†æ­£å¸¸å·¥ä½œ
- âœ… ç¼“å­˜æœºåˆ¶æœ‰æ•ˆï¼ˆå‡å°‘é‡å¤è°ƒç”¨ï¼‰
- âœ… é”™è¯¯é‡è¯•æœºåˆ¶å¯é 
- âœ… æˆæœ¬æ§åˆ¶åœ¨é¢„æœŸèŒƒå›´å†…

### æ•´ä½“é›†æˆ

- âœ… ç«¯åˆ°ç«¯æµç¨‹é¡ºç•…
- âœ… å¤„ç†é€Ÿåº¦ï¼š> 50 æ–‡ä»¶/åˆ†é’Ÿ
- âœ… å†…å­˜å ç”¨ï¼š< 500MB
- âœ… é”™è¯¯æ—¥å¿—æ¸…æ™°

---

## ğŸ“š å‚è€ƒèµ„æº

- [OpenAI Embeddings Guide](https://platform.openai.com/docs/guides/embeddings)
- [tree-sitter Rust Bindings](https://docs.rs/tree-sitter/latest/tree_sitter/)
- [Rust Async Book](https://rust-lang.github.io/async-book/)

---

**æœ€åæ›´æ–°**ï¼š2025-10-01  
**ä½œè€…**ï¼šAI Assistant  
**çŠ¶æ€**ï¼šå¾…å®æ–½
